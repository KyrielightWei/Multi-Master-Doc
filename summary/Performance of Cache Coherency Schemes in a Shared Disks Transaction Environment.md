### Performance of Cache Coherency Schemes in a Shared Disks Transaction Environment
韩国岭南大学 1997 年发表在 IEEE。

### 背景
针对存储分离场景、共享磁盘架构的存储系统（shared disks environment, SDE），每个计算节点之间的缓存常用锁来保证一致性，锁的粒度的不同，导致的网络开销也不一样，细粒度的锁带来的网络开销要比粗粒度锁大得多，但是并发度会更高。  

这篇文章集中在细粒度锁，相比之前的细粒度锁方案，本文提出的解决方法带来的网络开销要更小。
>本文提到的锁的粒度粗细可以用 page lock 和 record lock来区分，每个页面包括多个记录，page是粗粒度的，record是细粒度的。

### 细粒度锁开销
record lock 的方案虽然带来更高的并发度，但是也会带来更多的网络消耗。主要体现在两个方面：  
1. 锁开销(locking overhead)，假定一个事务涉及一个page中的五条record, 在page lock下，只需要针对这个page发送锁请求和接受锁相应就行；但是在 record lock下，就需要五次请求和响应。
2. 一致性控制开销(coherency control overhead)由于每个record在每个节点上都有可能存在，当一个节点 N1 修改每个记录 record1 时，其他节点需要将相应记录失效，且其他节点下次读取该记录时，需要从远程读取新的副本。

### 参照对象
##### 细粒度锁下实现一致性的两种方案：
1. merging scheme，每个节点都能对同一个记录更新，但是刷到磁盘前需要合并更新（由哪个节点更新，怎么更新，在引用[1]论文里详细讲解），消耗大量CPU资源。
2. write token scheme（WTS），采用 physical lock （P lock）方案，想更新数据只能先获取锁，且该锁只能由一个节点持有，其他节点想读这个页，由持有该页锁的节点发送副本到读节点。这样可能产生页面拷贝带来的网络开销。
>个人理解 WTS 就是粗粒度锁，虽然它也能适用于细粒度锁方案，是个最简单粗暴的方案。

##### 针对锁而言，前人做过的工作：
1. 锁保留 lock caching，某个节点在执行完某个事务之后，不会立即释放之前事务执行时申请的锁，而是等到其他节点需要这把锁时，才会将锁释放。这样在持有锁的这段时间，节点本地不用多次向锁管理器多次申请锁。
2. 锁降级 lock de-escalation，事务执行时总是尽可能申请最高级别的锁（比如页面的写锁），这样就会尽可能减少后续本地向所管理器申请锁的开销。当其他节点需要该锁时，本节点会检查锁的冲突，对持有的锁进行“降级”操作（比如其他节点需要修改页面 P1 上的 记录 R1, 而当前本地节点 N1 需要修改 P1 上的 R2，这样情况下，N1 会释放对整个页面 P1的写锁，降级为对 记录 R2 的 IX 锁）。

### 本文贡献
本文的主要贡献就是提出了一个叫做 Cache Coherency Schemes (CCS) 的方案，在提高多节点缓存读写并发度的同时，减少通信开销（相对于WTS来说）；并进一步结合上述两种锁（锁保留、锁降级）提出了改进的 ECCS方案，最后结合WTS，对三种方案进行了对比。

### 主要内容

#### 架构
本文采用共享磁盘的架构，上层是多个计算实例，下层是多个储存件构成的统一的存储系统，由一个全局的节点 global lock manage(GLM) 负责锁的管理（除了锁管理，这个节也会跟踪页面的有效副本所在位置）。每个计算节点都有一个本地缓存，采用LRU替换策略。  
每个页面都有一个 page_LSN 标记页面版本信息。  
GLM维护两个表： 1. 逻辑锁表，标记哪些节点上的事务正对哪些 record 申请了锁。 2. 物理锁表，保存 page_LSN、page owner节点、某个页面当前存在于哪些节点的集合(copy set)。

#### CCS：
相对于 WTS 举个简单的例子，假设页面PA包含三个记录R1，R2和R3，事务T1申请了更新了R1后，提交时 GLM 里PA 的LSN 更新；之后另一个节点N2上事务 T2 需要更新PA 的 R2（假设N2也缓存了PA）,此时N2检查 `本地LSN(PA) < GLM 上LSN(PA)`，根据WTS方案，页面 PA是要从N1复制到N2的，但很明显 N1和N2 上R2是一样的，这次复制完全多余。

CCS在GLM上维护了另一个表 record cache table(RCT)，形式如下：[record id, page-LSN of a page that includes the record]。
有了这张表就能解决上述例子问题，减少页面拷贝。

事务执行时，会申请记录 Ri 的锁，一旦得到了该锁，会产生以下几种情况：
1. S lock & page_LSN(RCT) = NULL: NULL 表明 record 之前从未被更改。如果 Ri 在当前节点，则读本地cache；若不在，则从其他节点复制过来这个page，或者从磁盘读过来（前面提到的copy set在这里发挥作用了），然后更新 copy set
2. S lock & page_LSN(PA) > page_LSN(RCT): 本地节点获取了读锁，且本地 PA 的LSN号大于 GLM RCT表里 PA的LSN号，直接读取本地 record就行。
3. S lock A page_LSN(PA) < pageLSN(RCT): 本地需要读的 Ri 版本过低，此时就会涉及到页面的拷贝。
4.  X lock: 首先页面的所有权会转移到当前节点（也可能已经在本地）；然后根据需要看page在哪里（本地，远程，磁盘），最后执行更改。

综上，CCS相对于WTS减少了部分page的拷贝，很多读写流程操作还是很类似的。

#### ECCS:
这里涉及对页面的五种上锁模式：IS,IX,S,SIX,X。
>IS 是对 record上读锁，IX 是对 record 上写锁；  
>结合“锁保留”理解，IX、IS需要在事务结束后释放，其他类型的锁则不需要释放，可以在事务结束后，将相应页面的锁保留在节点本地，直到其他节点需要才进行释放或“降级”；  
>按我们通俗理解，这几种锁级别依次增高；当节点拥有某个页面的 X 锁后，对该页面任何其他类型的锁请求都不需要向GLM申请，只需要本地处理即可。

关于一个节点N1在持有某个所类型的情况下，另一个节点Ni对同一页面申请锁时，N1 所持有的锁的类型状态转换详细参考节 3.3.1 节 table1。

类似CCS，事务执行时，会申请记录 Ri 的锁，一旦得到了该锁，会产生以下几种情况：
1. S lock & P-Lock = S or SIX: 若page_LSN(PA) == page_LSN(GLM)，GLM 授予锁给请求节点，然后请求的页面在本地就在本地读，不在本地就从远程或磁盘读过来；
2.  S lock & P-Lock = IS or IX: 这个和CCS类似，拿 page_LSN(PA) 和 pageLSN(RCT)比较，再进行类似CCS的操作。
3.  X lock: 和 CCS类似。

#### CCS和ECCS区别：
1. CCS每次事务执行都需要向GLM申请record级别锁；ECCS首先会看当前本地持有锁的类型，如果执行事务请求页面的锁已经在本地，且权限足够，则省去了一次和GLM的锁申请操作。
2. 不同于 CCS，ECCS申请读锁后，不是一味看 page_LSN(RCT)来判断是否要拷贝页面的，而是根据所升级后的类型，若是 S or SIX，这样的情况下不会比较page_LSN(RCT)，而是直接比较本地 page_LSN(PA)和 page_LSN(GLM)，若本地LSN小，直接拷贝整个页面过来，后续当前节点的事务对该页面任何记录的读都可以在本地进行，包括申请读锁。


### 实验
1. 访问数据倾斜，80%的事务只访问20%的数据。三种方案的性能差异对比，实验设置，同时调整读写比例，写比例在0-0.5之间。很明显，实验结果中，ECCS在写负载低的情况下，性能完爆其他两种方案。
2. 在数据分区很好的情况下，三种方案对比差异更明显，ECCS大幅减少了锁回调（其他节点从锁持有节点获取锁）的概率，在写负载0-0.5，都要好于其他方案。
3. UNIFORM Workload。将磁盘访问比例设置为 10% ，实验表明随着写入比例的提高，内存-磁盘的交换越来越频繁，ECCS在写比例低时，还能有比较好的性能，随着写比例提升，内存-磁盘的交换越频繁，性能会急剧下降。


### 结语
论文提到的细粒度锁方案，我们之前也讨论过，在实验中大部分场景下会比粗粒度锁方案有更好的性能；同时“锁保留”、“锁降级”方案在实验结果中也表明写负载低时，会有很好的性能收益，值得借鉴。  

只是论文实验用到了一个全局锁管理器 GLM，有点类似 DB2 方案，这个会不会造成性能瓶颈，在实验中没有体现出来。

#### Pull request #8 提到的几个疑问：
1. 修改后版本号如何变更？简单递增？若是假设每次修改版本号+1，那么两个节点对同一页上的不同行做并发修改时，可以申请到不同的x锁执行修改，修改后两个节点的页实际上内容不同，但版本号却是相同的，这种情况该如何处理?
>首先在 WTS 和 CCS 方案是不存在这种情况的，因为这两种方案没有 IX、IS锁，写锁只能由GLM同时分配给一个节点；  
>至于 ECCS 方案，虽然可以存在同一个页面PA 的不同 record R1、R2的写锁 IX 在不同节点持有的情况，但是从论文给出的信息来看，后续的实验不会存在这种情况，同时修改应该是不存在的，写操作应该类似WTS、CCS，原因如下：
>- - 节 `3.3.2 Extended CCS` 里第 3 种情况最后一句话：`Ni becomes the page owner of PA`，只有 page owner 是有权限写的。
>   - 节 `3.3.2 Extended CCS` 节尾：`By transferring the most recent page, it is guaranteed that ev- ery record of the page is recent.` GLM 里最新 page_LSN 只可能对应一个 page版本，不可能 1 个 page_LSN 对应多个 page 版本。（除非这里 GLM 在不同节点提交事务时做了一些判断和操作，来消除我们这个疑问，但是文章里没提。若真是在 GLM做了一些事情，具体是怎么做的，有待后续讨论）
2. RCT保存了page_LSN，物理锁表页保存了page_LSN，这两个冗余的LSN是否需要保持一致？是否会出现同一个页上的不同的行对应的page_LSN不同的情况？若是有，如何保持各个节点独立更新的page_LSN保持全局有序？
> 首先看计算节点和 GLM 保存了哪些有关LSN的信息：
> - 节点 ：page_LSN
> - GLM ：物理锁表里 [recent page_LSN, page owner, nodes with this recent page_LSN page]RCT[(R1,page_LSN),(R2,page_LSN)···(Rn,page_LSN)]  
> 
> RCT 保存单个 record 对应的最新版本，物理锁表保存的是最新的页面版本。  
> - 假设当前 N1、N2 两个节点都有相同版本号 1 的页面PA，包含两个记录[R1,R2];   
> - 当节点N1更新PA的R1时，会将page_LSN增加为2，事务提交时，GLM 将 RCT改为[(R1,2),(R2,1)], 同时物理锁表对于PA的记录应是[2, N1, {N1}]   
> - 节点N2读取 R2, 申请读锁，GLM对于PA记录改为[2, N1, {N1，N2}]，N2 发现 RCT 中 R2版本号为1 ，恰好本地 page_LSN(PA)也是 1，不需要做页面的拷贝，直接读取本地。   
> - 节点N2修改 R2，申请页面PA的写锁，GLM 返回的PA最新版本是2，而节点 page_LSN(PA) 是 1，需要从 N1 将页面拷贝过来，此时page_LSN(PA) 变为 2，然后进行修改操作；事务提交后，GLM 对于PA的记录应是[3, N2, {N2}]， RCT改为[(R1,2),(R2,3)]。 
>
3. 又不是你写的论文....实验部分为啥只说方案好的方面，我也很想知道差的时候会差到什么程序，最好可以附上一些关键的实验图表进行说明
>性能差时，ECCS、CCS并不会有什么优势，和粗暴的WTS表现几乎无异。下降程度从 100-200 TPS 降至 30 TPS左右。